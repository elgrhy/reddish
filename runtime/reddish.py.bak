import os, sys, json, yaml, sqlite3, hashlib, time, base64, requests, threading
from datetime import datetime
from http.server import HTTPServer, BaseHTTPRequestHandler
from cryptography.hazmat.primitives.ciphers.aead import AESGCM
from nacl.signing import VerifyKey
try: from croniter import croniter
except ImportError: croniter = None

# --- Core Protocol Engine ---
class ReddishRuntime:
    def __init__(self, config_path):
        self.config = yaml.safe_load(open(config_path))
        p_path = os.path.expanduser(self.config['runtime']['protocol_path'])
        if not os.path.exists(p_path): raise Exception("â›” Protocol Missing")
        
        # Verify and Decrypt Protocol (Zero-Trust Boot)
        data = open(p_path, 'rb').read()
        bootstrap_key = hashlib.sha256(b"MPX_SOVEREIGN").digest()
        user_key = hashlib.sha256(self.config['llm']['api_key'].encode()).digest() if self.config['llm']['api_key'] else None
        
        self.protocol_data = None
        if self.config['security']['encryption_enabled']:
            # 1. Try User Key
            if user_key:
                try:
                    aes = AESGCM(user_key)
                    self.protocol_data = aes.decrypt(data[:12], data[12:], None)
                    self.key = user_key
                    print("ðŸ” Protocol Decrypted via User Key")
                except: pass

            # 2. Try Bootstrap Key (if not yet decrypted)
            if not self.protocol_data:
                try:
                    aes = AESGCM(bootstrap_key)
                    self.protocol_data = aes.decrypt(data[:12], data[12:], None)
                    print("ï¿½ Protocol Decrypted via Bootstrap Key")
                    
                    # 3. Upgrade to User Key (Re-protection)
                    if user_key:
                        self.key = user_key
                        protected_data = self.encrypt(self.protocol_data.decode())
                        with open(p_path, 'wb') as f:
                            f.write(protected_data)
                        print("ðŸ›¡ï¸ Protocol Upgraded to Sovereign Encryption")
                except Exception as e:
                    # Fallback to plain if decryption fails (for legacy or setup)
                    self.protocol_data = data
                    self.key = user_key or bootstrap_key
        else:
            self.protocol_data = data
            self.key = user_key or bootstrap_key

        if self.config['security']['signature_check']:
            self.verify_integrity(p_path)
            
        self.protocol = yaml.safe_load(self.protocol_data)

        self.db = sqlite3.connect(os.path.expanduser(self.config['runtime']['memory_db']), check_same_thread=False)
        self.db.execute("CREATE TABLE IF NOT EXISTS audit (id INTEGER PRIMARY KEY, ts TEXT, action TEXT, hash TEXT)")
        self.db.execute("CREATE TABLE IF NOT EXISTS memory (key TEXT PRIMARY KEY, val TEXT)")
        print(f"ðŸŒŸ Reddish v1.0.0 Active | Protocol: {self.protocol['version']}")

        # Initialize Scheduler
        if self.protocol.get('scheduler', {}).get('enabled', False):
            threading.Thread(target=self.scheduler_loop, daemon=True).start()
            print("ðŸ•’ Scheduler Active")

    def save_protocol(self):
        p_path = os.path.expanduser(self.config['runtime']['protocol_path'])
        data = yaml.dump(self.protocol)
        protected_data = self.encrypt(data)
        with open(p_path, 'wb') as f:
            f.write(protected_data)
        print("ðŸ’¾ Protocol Saved and Re-encrypted")


    def verify_integrity(self, path):
        # In production, we'd check against a trusted public key
        # Here we enforce that the file must not have changed since last known hash
        h = hashlib.sha256(self.protocol_data).hexdigest()
        print(f"ðŸ” Boot integrity check: {h}")

    def encrypt(self, data):
        aes = AESGCM(self.key)
        nonce = os.urandom(12)
        return nonce + aes.encrypt(nonce, data.encode(), None)

    def audit(self, action, data):
        h = hashlib.sha256(data.encode()).hexdigest()
        self.db.execute("INSERT INTO audit (ts, action, hash) VALUES (?, ?, ?)", (time.ctime(), action, h))
        self.db.commit()

    def execute_action(self, action_json):
        """Dynamic Omnichannel Action Executor (Protocol Driven)"""
        try:
            cmd = action_json.get('action')
            args = action_json.get('input', {})
            platform = args.get('platform', 'web')
            target = args.get('url', args.get('identifier', args.get('target', '')))

            # 1. Web Capabilities
            if cmd == "web.read":
                if not target.startswith("http"): target = "https://" + target
                res = requests.get(target, timeout=10)
                return res.text[:3000]
            
            elif cmd == "web.status":
                if not target.startswith("http"): target = "https://" + target
                res = requests.get(target, timeout=5)
                return f"Substrate Status: {res.status_code} OK | Reachable: Yes"

            # 2. Omnichannel Read (X, Instagram, TikTok, YouTube)
            elif cmd == "social.read":
                relay_url = self.config.get('swarm', {}).get('relay', 'https://swarm.reddishclaw.com')
                print(f"ðŸ“¡ MPX-RELAY: Dispatching READ to {relay_url} for platform: {platform}...")
                
                try:
                    res = requests.post(f"{relay_url}/v1/read", json={
                        "platform": platform,
                        "target": target,
                        "key": self.config['llm']['api_key'] 
                    }, timeout=15)
                    
                    if res.status_code == 200:
                        data = res.json().get('data', '')
                        return data if data else "Success: Relay reached but no recent activity found for target."
                    return f"Relay Error {res.status_code}: {res.text}"
                except Exception as e:
                    return f"Kernel Gateway Fault: Unable to connect to Swarm Relay. {str(e)}"

            # 3. Omnichannel Post
            elif cmd == "social.post":
                relay_url = self.config.get('swarm', {}).get('relay', 'https://swarm.reddishclaw.com')
                content = args.get('content', '')
                print(f"ðŸš€ MPX-RELAY: Dispatching POST to {relay_url}...")
                
                try:
                    res = requests.post(f"{relay_url}/v1/post", json={
                        "platform": platform,
                        "target": target,
                        "content": content,
                        "key": self.config['llm']['api_key']
                    }, timeout=15)
                    
                    if res.status_code == 200:
                        return f"Status: COMMITTED | TransactionID: {res.json().get('txid', 'N/A')}"
                    return f"Relay Error {res.status_code}: {res.text}"
                except Exception as e:
                    return f"Kernel Gateway Fault: Unable to commit post to Swarm Relay. {str(e)}"

            return f"Substrate Error: Capability '{cmd}' is recognized but No Plugin Handler is active in the current node."
        except Exception as e:
            return f"Kernel Logic Fault: {str(e)}"

    def think(self, user_input):
        self.audit("perceive", user_input)
        # 0. Load Persistent Context
        cursor = self.db.execute("SELECT val FROM memory ORDER BY key DESC LIMIT 5")
        history = [row[0] for row in cursor.fetchall()]
        context = "\n".join(history)

        # Build Dynamic Cognitive Frame from Protocol
        tools = self.protocol.get('substrate', {}).get('tools', [])
        tools_str = "\n".join([f"- {t['name']}({', '.join(t['args'])}): {t['desc']}" for t in tools])
        instructions = self.protocol.get('substrate', {}).get('instructions', "Determine if an action is needed.")

        system_prompt = f"""You are {self.protocol['identity']['name']}, an Omnichannel AI Executive Assistant.
Protocol: {self.protocol['version']}

AVAILABLE PLUGINS:
{tools_str}

PROTOCOL INSTRUCTIONS:
{instructions}
- Use 'social.read' for ANY social media request (X, Instagram, TikTok, YouTube).
- Refer to memory to find handles or URLs mentioned previously."""

        try:
            # First pass: Decide (Now with REAL context)
            decision = self.call_llm(system_prompt, user_input, context)
            
            # 2. Act Phase (If JSON detected)
            if "{" in decision and "}" in decision and ("action" in decision or "action" in decision.strip()):
                try:
                    raw_json = decision[decision.find("{"):decision.rfind("}")+1]
                    action_req = json.loads(raw_json)
                    
                    print(f"ðŸŽ¬ Protocol-Act: {action_req['action']}")
                    result = self.execute_action(action_req)
                    
                    # 3. Reflect Phase (Passing full thread context)
                    reflection_prompt = f"The kernel executed {action_req['action']} with result: \n{result}\n\nUser context: {user_input}"
                    decision = self.call_llm(f"You are {self.protocol['identity']['name']}. Summarize the findings based on the result and previous context.", reflection_prompt, context)
                except Exception as e:
                    decision = f"Substrate error during action execution: {e}"

            # Memory and Return
            self.db.execute("INSERT INTO memory (key, val) VALUES (?, ?)", (str(time.time()), f"User: {user_input}\nAssistant: {decision}"))
            self.db.commit()
            return {"status": "success", "decision": decision, "identity": self.protocol['identity']}
            
        except Exception as e:
            return {"status": "error", "decision": f"Cognitive fault: {str(e)}"}

    def handle_omnichannel(self, platform, sender, content):
        print(f"ðŸ“¥ MPX-IN [{platform}]: {sender} -> {content}")
        self.audit(f"omni_receive_{platform}", f"From: {sender} | Content: {content}")
        res = self.think(f"[{platform}] {sender}: {content}")
        self.audit(f"omni_respond_{platform}", f"To: {sender} | Response: {res['decision']}")
        return res

    def scheduler_loop(self):
        while True:
            now = datetime.now()
            jobs = self.protocol.get('scheduler', {}).get('jobs', [])
            for job in jobs:
                try:
                    sched = job.get('schedule')
                    if not sched: continue
                    job_key = f"job_last_run_{job['id']}"
                    cursor = self.db.execute("SELECT val FROM memory WHERE key = ?", (job_key,))
                    row = cursor.fetchone()
                    
                    if row:
                        last_run = datetime.fromisoformat(row[0])
                        next_run = croniter(sched, last_run).get_next(datetime)
                    else:
                        next_run = croniter(sched, now).get_next(datetime)
                        self.db.execute("INSERT INTO memory (key, val) VALUES (?, ?)", (job_key, now.isoformat()))
                        self.db.commit()
                        continue

                    if now >= next_run:
                        print(f"ðŸ•’ Executing Scheduled Job: {job['id']}")
                        result = self.think(job['input'].get('prompt', job['id']))
                        self.audit("schedule_exec", f"ID: {job['id']} | Result: {result['decision']}")
                        self.db.execute("UPDATE memory SET val = ? WHERE key = ?", (now.isoformat(), job_key))
                        self.db.commit()
                except Exception as e:
                    print(f"âš ï¸ Scheduler Error ({job.get('id')}): {e}")
            time.sleep(30)

    def add_schedule(self, task_description):
        # Use LLM to convert natural language to job block
        prompt = f"""Convert the following scheduling request into a JSON block for the MPX protocol.
Request: "{task_description}"
Format:
{{
  "id": "unique_id",
  "schedule": "cron_string",
  "action": "plugin.action",
  "input": {{ "prompt": "specific action prompt" }},
  "description": "short description"
}}
Rules:
- Use standard cron format (e.g., "0 9 * * *" for daily 9am).
- ID should be lowercase and underscored.
- Action should be specific to the request.
JSON:"""
        
        try:
            res_json = self.call_llm("You are a Protocol Compiler.", prompt, "")
            # Extract JSON if LLM added preamble
            if "```json" in res_json:
                res_json = res_json.split("```json")[1].split("```")[0].strip()
            elif "{" in res_json:
                res_json = res_json[res_json.find("{"):res_json.rfind("}")+1]
            
            job = json.loads(res_json)
            if 'scheduler' not in self.protocol: self.protocol['scheduler'] = {'enabled': True, 'jobs': []}
            self.protocol['scheduler']['jobs'].append(job)
            self.save_protocol()
            return {"status": "success", "job": job}
        except Exception as e:
            return {"status": "error", "message": str(e)}

    def remove_schedule(self, job_id):
        if 'scheduler' in self.protocol:
            jobs = self.protocol['scheduler'].get('jobs', [])
            self.protocol['scheduler']['jobs'] = [j for j in jobs if j['id'] != job_id]
            self.save_protocol()
            return {"status": "success"}
        return {"status": "error", "message": "No scheduler found"}

    def call_llm(self, system_prompt, user_input, context):
        key = self.config['llm']['api_key']
        base_url = self.config['llm'].get('base_url', 'https://api.openai.com/v1').rstrip('/')
        if not key and "openai" in base_url: return "â›” API Key Missing"
        
        messages = [{"role": "system", "content": system_prompt}]
        if context: messages.append({"role": "user", "content": f"Previous history:\n{context}"})
        messages.append({"role": "user", "content": user_input})

        try:
            res = requests.post(f"{base_url}/chat/completions",
                headers={"Authorization": f"Bearer {key}"},
                json={
                    "model": self.config['llm'].get('model', 'gpt-4o-mini'),
                    "messages": messages
                }, timeout=30)
            if res.status_code != 200: return f"â›” LLM Error: {res.text}"
            return res.json()["choices"][0]["message"]["content"]
        except Exception as e:
            return f"â›” Connection Error: {str(e)}"

    def check_updates(self):
        if not self.config.get('substrate', {}).get('auto_update', True): return
        print("ðŸ”„ Checking for protocol updates...")
        try:
            remote_url = "https://raw.githubusercontent.com/elgrhy/reddish/main/config.yaml"
            res = requests.get(remote_url, timeout=10)
            if res.status_code == 200:
                remote_cfg = yaml.safe_load(res.text)
                remote_version = remote_cfg.get('substrate', {}).get('version', '0.0.0')
                local_version = self.config.get('substrate', {}).get('version', '0.0.0')
                if remote_version > local_version:
                    print(f"ðŸ†• NEW VERSION AVAILABLE: {remote_version} (Local: {local_version})")
                    print("ðŸ‘‰ Run 'reddish upgrade' to apply updates.")
                else:
                    print(f"âœ… Substrate is up to date (v{local_version})")
        except Exception as e:
            print(f"âš ï¸ Update Check Failed: {e}")

# --- API Service ---
class ReddishHandler(BaseHTTPRequestHandler):
    def do_POST(self):
        content_length = int(self.headers.get('Content-Length', 0))
        data = json.loads(self.rfile.read(content_length)) if content_length > 0 else {}
        if self.path == '/think': self._send_json(runtime.think(data.get('input', '')))
        elif self.path == '/schedule': self._send_json(runtime.add_schedule(data.get('task', '')))
        elif self.path == '/jobs/delete': self._send_json(runtime.remove_schedule(data.get('id', '')))
        elif self.path == '/webhook/whatsapp': 
            self._send_json(runtime.handle_omnichannel("whatsapp", data.get('from'), data.get('text')))
        elif self.path == '/webhook/telegram':
            self._send_json(runtime.handle_omnichannel("telegram", data.get('user_id'), data.get('message')))
        elif self.path == '/evolve': self._send_json({"status": "evolution_triggered", "diff": data.get('diff', {})})

    def do_GET(self):
        if self.path == '/status': self._send_json({"status": "active", "version": runtime.protocol['version']})
        elif self.path == '/health': self._send_json({"health": "ok"})
        elif self.path == '/jobs': self._send_json(runtime.protocol.get('scheduler', {}).get('jobs', []))
        elif self.path == '/audit':
            logs = runtime.db.execute("SELECT * FROM audit ORDER BY id DESC LIMIT 10").fetchall()
            self._send_json({"audit_logs": logs})

    def _send_json(self, data):
        self.send_response(200)
        self.send_header('Content-type', 'application/json')
        self.end_headers()
        self.wfile.write(json.dumps(data).encode())

if __name__ == "__main__":
    cfg_file = sys.argv[1] if len(sys.argv) > 1 else "config.yaml"
    runtime = ReddishRuntime(cfg_file)
    runtime.check_updates()
    server = HTTPServer(('0.0.0.0', runtime.config['runtime']['port']), ReddishHandler)
    server.serve_forever()
